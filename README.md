

## 📰 Building a News ETL Data Pipeline with Python and SQLite

In this project, we’ll create an **ETL pipeline** that extracts news data from the **News API**, transforms it into a structured format, and loads it into an **SQLite** database. We’ll also automate the pipeline using **Apache Airflow** to run the entire process on a schedule. Ready to dive into data engineering with Python? Let’s go! 🚀📊

## Project Overview

The goal of this project is to build a complete **ETL pipeline** that extracts news data from the **News API**, processes it, and stores it in an **SQLite database**. We’ll transform the semi-structured JSON data from the API into a structured format that’s ready for storage and analysis. 

To automate the entire process, we’ll use **Apache Airflow** to schedule and manage the ETL pipeline, making it easy to run regularly and ensure that the data stays updated.

### Key Features:

- 📰 **News Data Extraction**: Extract the latest news data from the **News API** in JSON format.
- 🔄 **Data Transformation**: Clean and transform the extracted data into a structured format using **pandas**.
- 🗄️ **Load into SQLite**: Store the transformed data in an **SQLite database** for further use and analysis.
- 🔄 **Pipeline Automation**: Automate the ETL pipeline using **Apache Airflow** to run at regular intervals.

## 🛠 Technologies Used

- **Python**: Core language for developing the ETL pipeline.
- **pandas**: For data cleaning and transformation.
- **SQLite**: Lightweight database for storing the transformed news data.
- **News API**: To fetch the latest news data in JSON format.
- **Apache Airflow**: For automating and scheduling the ETL pipeline.

## 🤖 Skills Applied

- Data Pipeline Engineering
- Data Extraction, Cleaning, and Transformation
- Python and SQLite
- Apache Airflow for Automation

## Example Tasks You Can Do

- **Extract News Data**: Use the News API to fetch the latest headlines and stories in JSON format.
- **Transform Data**: Process and clean the JSON data, structuring it for database storage.
- **Load to SQLite**: Store the cleaned data in an SQLite database for further analysis.
- **Automate with Airflow**: Schedule and automate the ETL pipeline to ensure data is always up-to-date.

