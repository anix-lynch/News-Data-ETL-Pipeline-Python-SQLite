

## ğŸ“° Building a News ETL Data Pipeline with Python and SQLite

In this project, weâ€™ll create an **ETL pipeline** that extracts news data from the **News API**, transforms it into a structured format, and loads it into an **SQLite** database. Weâ€™ll also automate the pipeline using **Apache Airflow** to run the entire process on a schedule. Ready to dive into data engineering with Python? Letâ€™s go! ğŸš€ğŸ“Š

## Project Overview

The goal of this project is to build a complete **ETL pipeline** that extracts news data from the **News API**, processes it, and stores it in an **SQLite database**. Weâ€™ll transform the semi-structured JSON data from the API into a structured format thatâ€™s ready for storage and analysis. 

To automate the entire process, weâ€™ll use **Apache Airflow** to schedule and manage the ETL pipeline, making it easy to run regularly and ensure that the data stays updated.

### Key Features:

- ğŸ“° **News Data Extraction**: Extract the latest news data from the **News API** in JSON format.
- ğŸ”„ **Data Transformation**: Clean and transform the extracted data into a structured format using **pandas**.
- ğŸ—„ï¸ **Load into SQLite**: Store the transformed data in an **SQLite database** for further use and analysis.
- ğŸ”„ **Pipeline Automation**: Automate the ETL pipeline using **Apache Airflow** to run at regular intervals.

## ğŸ›  Technologies Used

- **Python**: Core language for developing the ETL pipeline.
- **pandas**: For data cleaning and transformation.
- **SQLite**: Lightweight database for storing the transformed news data.
- **News API**: To fetch the latest news data in JSON format.
- **Apache Airflow**: For automating and scheduling the ETL pipeline.

## ğŸ¤– Skills Applied

- Data Pipeline Engineering
- Data Extraction, Cleaning, and Transformation
- Python and SQLite
- Apache Airflow for Automation

## Example Tasks You Can Do

- **Extract News Data**: Use the News API to fetch the latest headlines and stories in JSON format.
- **Transform Data**: Process and clean the JSON data, structuring it for database storage.
- **Load to SQLite**: Store the cleaned data in an SQLite database for further analysis.
- **Automate with Airflow**: Schedule and automate the ETL pipeline to ensure data is always up-to-date.

